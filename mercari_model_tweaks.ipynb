{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the Mercari children's book dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.chdir('/Users/mleong/github/used-books')\n",
    "books = pd.read_csv('used-book-data.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .str accessor with string values, which use np.object_ dtype in pandas",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-6b8899a583e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# price: modify to eliminate '$' sign and convert to number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mbooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mbooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'$'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mbooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5061\u001b[0m         if (name in self._internal_names_set or name in self._metadata or\n\u001b[1;32m   5062\u001b[0m                 name in self._accessors):\n\u001b[0;32m-> 5063\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5064\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5065\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/accessor.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;31m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0maccessor_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0;31m# Replace the property with the accessor object. Inspired by:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m# http://www.pydanny.com/cached-property.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/strings.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1796\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1797\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_categorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/strings.py\u001b[0m in \u001b[0;36m_validate\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1816\u001b[0m             \u001b[0;31m# (instead of test for object dtype), but that isn't practical for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1817\u001b[0m             \u001b[0;31m# performance reasons until we have a str dtype (GH 9343)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1818\u001b[0;31m             raise AttributeError(\"Can only use .str accessor with string \"\n\u001b[0m\u001b[1;32m   1819\u001b[0m                                  \u001b[0;34m\"values, which use np.object_ dtype in \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1820\u001b[0m                                  \"pandas\")\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .str accessor with string values, which use np.object_ dtype in pandas"
     ]
    }
   ],
   "source": [
    "# eliminate duplicate rows\n",
    "books = books.drop_duplicates()\n",
    "\n",
    "# id: convert to string\n",
    "books.id = books.id.astype(str)\n",
    "\n",
    "# posted: convert to datetime\n",
    "books.posted = pd.to_datetime(books.posted)\n",
    "\n",
    "# sold: combine buy now and add to cart categories to be 'for sale'\n",
    "books = books.replace('Buy now', 'available')\n",
    "\n",
    "# price: modify to eliminate '$' sign and convert to number\n",
    "books['price'] = books['price'].str.replace(',', '')\n",
    "books['price'] = books['price'].str.replace('$', '')\n",
    "books['price'] = books['price'].astype(int)\n",
    "\n",
    "# create new column \"shipping_cost\" that is continuous variable from $0 to whatever shipping fee is\n",
    "def shipping_cost(shipping_string):\n",
    "    shipping_after = shipping_string.split(\" | \")\n",
    "    if 'Free' in shipping_after[0]:\n",
    "        shipping_cost = 0\n",
    "    elif '$' in shipping_after[0]:\n",
    "        shipping_cost = shipping_after[0]\n",
    "        shipping_cost = shipping_cost.replace('$', '') \n",
    "    else:\n",
    "        shipping_cost = None\n",
    "    return shipping_cost\n",
    "books['shipping_cost'] = books['shipping'].apply(shipping_cost)\n",
    "books.shipping_cost = pd.to_numeric(books.shipping_cost, errors='coerce')\n",
    "\n",
    "# create new simpler column \"shipping_time\" that is number of days to arrival\n",
    "def shipping_time(shipping_string):\n",
    "    if 'days' in shipping_string:\n",
    "        shipping_after = shipping_string.split(\" | \")\n",
    "        if 'days' in shipping_after[0]:\n",
    "            shipping_time = shipping_after[0]\n",
    "            shipping_time = shipping_time.replace('+ days', '')\n",
    "        elif 'days' in shipping_after[1]:\n",
    "            shipping_time = shipping_after[1]\n",
    "            shipping_time = shipping_time.replace('+ days', '')\n",
    "        else:\n",
    "            shipping_time = None\n",
    "    else:\n",
    "        shipping_time = None\n",
    "    return shipping_time\n",
    "books['shipping_time'] = books['shipping'].apply(shipping_time)\n",
    "\n",
    "# create new simpler column \"shipping_location\" that is just the state product is coming from\n",
    "def shipping_location(shipping_string):\n",
    "    if 'from' in shipping_string:\n",
    "        shipping_after = shipping_string.split(\" | \")\n",
    "        if 'from' in shipping_after[0]:\n",
    "            shipping_location = shipping_after[0]\n",
    "            shipping_location = shipping_location.replace('from ', '')\n",
    "        elif 'from' in shipping_after[1]:\n",
    "            shipping_location = shipping_after[1]\n",
    "            shipping_location = shipping_location.replace('from ', '')\n",
    "        elif 'from' in shipping_after[2]:\n",
    "            shipping_location = shipping_after[2]\n",
    "            shipping_location = shipping_location.replace('from ', '')\n",
    "        else:\n",
    "            shipping_location = None\n",
    "    else:\n",
    "        shipping_location = None\n",
    "    return shipping_location\n",
    "books['shipping_location'] = books['shipping'].apply(shipping_location)\n",
    "\n",
    "# total price: new column that is sum of price and shipping fee\n",
    "books['total_price'] = books['price']+books['shipping_cost']\n",
    "\n",
    "# Making free_shipping column a yes/no\n",
    "def free_shipping(shipping_cost):\n",
    "    if shipping_cost == 0:\n",
    "        free_shipping = 1\n",
    "    else:\n",
    "        free_shipping = 0\n",
    "    return free_shipping\n",
    "books['free_shipping'] = books.shipping_cost.apply(free_shipping)\n",
    "\n",
    "# Making brand_included feature thats just a yes or no\n",
    "books['brand_included'] = books.brand.notnull()\n",
    "zeroANDones = lambda x: x*1\n",
    "books.brand_included = books.brand_included.apply(zeroANDones)\n",
    "\n",
    "# New column sold_true\n",
    "def sold_true(status):\n",
    "    if status == 'SOLD':\n",
    "        sold_true = 1\n",
    "    else:\n",
    "        sold_true = 0\n",
    "    return sold_true\n",
    "books['sold_true'] = books.sold.apply(sold_true)\n",
    "\n",
    "# New column summing of description length\n",
    "def description_length(desc_string):\n",
    "    description_length = len(desc_string)\n",
    "    return description_length\n",
    "books['description_length'] = books.description.apply(description_length)\n",
    "\n",
    "# New column summing of title length\n",
    "def title_length(title_string):\n",
    "    title_length = len(title_string)\n",
    "    return title_length\n",
    "books['title_length'] = books.title.apply(title_length)\n",
    "\n",
    "# New column condition_ordinal from new = 1, like new = 2, good = 3, fair = 4, poor = 5\n",
    "def condition_ordinal(condition):\n",
    "    if condition == \"New\":\n",
    "        condition_ordinal = 1\n",
    "    elif condition == \"Like new\":\n",
    "        condition_ordinal = 2\n",
    "    elif condition == \"Good\":\n",
    "        condition_ordinal = 3\n",
    "    elif condition == \"Fair\":\n",
    "        condition_ordinal = 4\n",
    "    elif condition == \"Poor\":\n",
    "        condition_ordinal = 5\n",
    "    else: condition_ordinal = None\n",
    "    return condition_ordinal\n",
    "books['condition_ordinal'] = books.condition.apply(condition_ordinal)\n",
    "\n",
    "# New column days_since_posted. Was scraped on 2019-06-10 so range from 0 and up.\n",
    "from datetime import date\n",
    "\n",
    "def days_since_posted(posted):\n",
    "    posted_day = date(posted.year, posted.month, posted.day)\n",
    "    scrape_day = date(2019, 6, 10)\n",
    "    days_since_posted = scrape_day - posted_day\n",
    "    return days_since_posted.days\n",
    "books['days_since_posted'] = books.posted.apply(days_since_posted)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# award winning people\n",
    "geiselpeople = pd.read_csv('geiselpeople2.csv', encoding = \"ISO-8859-1\")\n",
    "geiselpeople = geiselpeople.drop_duplicates()\n",
    "geiselpeoplelist = geiselpeople['people'].tolist()\n",
    "\n",
    "# amazon bestsellers\n",
    "amazon = pd.read_csv('amazon.csv')\n",
    "amazon = amazon.drop_duplicates()\n",
    "amazonlist = amazon['amazon'].tolist()\n",
    "\n",
    "# popular brands\n",
    "popularbrands = ['disney','pixar', 'frozen', 'moana', 'toy story',\n",
    "                 'americangirl', 'american girl',\n",
    "                 'dr seuss', 'dr. seuss', 'drseuss', 'dr.seuss',\n",
    "                 'lego', 'sesame street', 'sesamestreet', 'elmo', 'cookie monster', 'big bird',\n",
    "                 'leap frog', 'leapfrog', 'minecraft', 'mine craft', 'marvel',\n",
    "                 'spider-man', 'spiderman', 'hulk', 'iron man', 'thor', 'captain america',\n",
    "                 'wolverine', 'black panther', 'doctor strange', 'captain marvel', 'black widow',\n",
    "                 'scarlet witch', 'hawkeye', 'daredevil', 'silver surfer', 'avengers', 'xmen',\n",
    "                 'x-men', 'deadpool', 'eric carle', 'hungry caterpillar', 'ericcarle', \n",
    "                 'nickelodean', 'starwars', 'star wars', 'jedi', 'yoda', 'darth vader', \n",
    "                 'winnie-the-pooh', 'winnie the pooh', 'tigger', 'eeyore', \n",
    "                 'harrypotter', 'harry potter', 'hogwarts', 'dumbledore', 'fanstastic beasts',\n",
    "                 'diary of a wimpy kid', 'wimpy kid'\n",
    "                ]\n",
    "# Creating megastring!\n",
    "def megastring(row):\n",
    "  return (str(row['brand']) + \" \" + str(row['title']) + \" \" + str(row['description'])).lower()\n",
    "\n",
    "books['megastring'] = books.apply(megastring, axis = 1)\n",
    "\n",
    "def geiselpeople(megastring):\n",
    "    \"\"\"The Theodor Seuss Geisel Award is a literary award \n",
    "    by the American Library Association (ALA) that annually \n",
    "    recognizes the \"author(s) and illustrator(s) of the most \n",
    "    distinguished book for beginning readers published in \n",
    "    English in the United States during the preceding year.\n",
    "    All winning and nomineed authors/illustrators listed here\n",
    "    \"\"\"\n",
    "    megalist = geiselpeoplelist\n",
    "    megalist = [x.lower() for x in megalist]\n",
    "    if any(word in megastring for word in megalist):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "books['awardees'] = books.megastring.apply(geiselpeople)\n",
    "\n",
    "def amazon(megastring):\n",
    "    \"\"\"amazon children's books best sellers list as of 18June2019.\n",
    "    Kind of biased towards father's day, summer holiday travel,\n",
    "    and summer holiday school work though\n",
    "    \"\"\"\n",
    "    megalist = amazonlist\n",
    "    megalist = [x.lower() for x in megalist]\n",
    "    if any(word in megastring for word in megalist):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "books['amazon'] = books.megastring.apply(amazon)\n",
    "\n",
    "def topbrands(megastring):\n",
    "    \"\"\"the most frequently listed mercari brands\n",
    "    and key words\n",
    "    \"\"\"\n",
    "    megalist = popularbrands\n",
    "    megalist = [x.lower() for x in megalist]\n",
    "    if any(word in megastring for word in megalist):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "books['topbrands'] = books.megastring.apply(topbrands)\n",
    "\n",
    "# Feature engineering like what\n",
    "def disney(megastring):\n",
    "    if any(word in megastring for word in ['disney','pixar', 'frozen', 'moana', 'toy story' ]): \n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "books['disney'] = books.megastring.apply(disney)\n",
    "\n",
    "def americangirl(megastring):\n",
    "    if any(word in megastring for word in ['americangirl', 'american girl']): \n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "books['americangirl'] = books.megastring.apply(americangirl)\n",
    "\n",
    "def seuss(megastring):\n",
    "    if any(word in megastring for word in ['dr seuss', 'dr. seuss', 'drseuss', 'dr.seuss']): \n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "books['seuss'] = books.megastring.apply(seuss)\n",
    "\n",
    "def lego(megastring):\n",
    "    if any(word in megastring for word in ['lego']): \n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "books['lego'] = books.megastring.apply(lego)\n",
    "\n",
    "def sesame(megastring):\n",
    "    if any(word in megastring for word in ['sesame street', 'sesamestreet', 'elmo', 'cookie monster', 'big bird']): \n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "books['sesame'] = books.megastring.apply(sesame)\n",
    "\n",
    "def leapfrog(megastring):\n",
    "    if any(word in megastring for word in ['leap frog', 'leapfrog']): \n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "books['leapfrog'] = books.megastring.apply(leapfrog)\n",
    "\n",
    "def minecraft(megastring):\n",
    "    if any(word in megastring for word in ['minecraft', 'mine craft']): \n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "books['minecraft'] = books.megastring.apply(minecraft)\n",
    "\n",
    "def marvel(megastring):\n",
    "    if any(word in megastring for word in ['spider-man', 'spiderman', 'hulk', 'iron man', 'thor', 'captain america',\n",
    "                                           'wolverine', 'black panther', 'doctor strange', 'captain marvel', 'black widow',\n",
    "                                           'scarlet witch', 'hawkeye', 'daredevil', 'silver surfer', 'avengers', 'xmen',\n",
    "                                           'x-men', 'deadpool']):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "books['marvel'] = books.megastring.apply(marvel)\n",
    "           \n",
    "def ericcarle(megastring):\n",
    "    if any(word in megastring for word in ['eric carle', 'hungry caterpillar', 'ericcarle']):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "books['ericcarle'] = books.megastring.apply(ericcarle)\n",
    "           \n",
    "def nickelodean(megastring):\n",
    "    if any(word in megastring for word in ['nickelodean']):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "books['nickelodean'] = books.megastring.apply(nickelodean)\n",
    "\n",
    "def starwars(megastring):\n",
    "    if any(word in megastring for word in ['starwars', 'star wars', 'jedi', 'yoda', 'darth vader']):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "books['starwars'] = books.megastring.apply(starwars)\n",
    "\n",
    "def winniethepooh(megastring):\n",
    "    if any(word in megastring for word in ['winnie-the-pooh', 'winnie the pooh', 'tigger', 'eeyore']):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "books['winniethepooh'] = books.megastring.apply(winniethepooh)\n",
    "\n",
    "def harrypotter(megastring):\n",
    "    if any(word in megastring for word in ['harrypotter', 'harry potter', 'hogwarts', 'dumbledore', 'fanstastic beasts']):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "books['harrypotter'] = books.megastring.apply(harrypotter)\n",
    "\n",
    "def wimpykid(megastring):\n",
    "    if any(word in megastring for word in ['diary of a wimpy kid', 'wimpy kid']):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "books['wimpykid'] = books.megastring.apply(wimpykid)\n",
    "\n",
    "def wimpykid(megastring):\n",
    "    if any(word in megastring for word in ['diary of a wimpy kid', 'wimpy kid']):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "books['wimpykid'] = books.megastring.apply(wimpykid)\n",
    "\n",
    "def fisherprice(megastring):\n",
    "    if any(word in megastring for word in ['fisherprice', 'fisher-price', 'fisher price']):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "books['fisherprice'] = books.megastring.apply(fisherprice)\n",
    "\n",
    "def scholastic(megastring):\n",
    "    if any(word in megastring for word in ['scholastic']):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "books['scholastic'] = books.megastring.apply(scholastic)\n",
    "\n",
    "def bundle(megastring):\n",
    "    if any(word in megastring for word in ['bundle', 'set of']):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "books['bundle'] = books.megastring.apply(bundle)\n",
    "\n",
    "def vintage(megastring):\n",
    "    if any(word in megastring for word in ['vintage']):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "books['vintage'] = books.megastring.apply(vintage)\n",
    "\n",
    "def smokefree(megastring):\n",
    "    if any(word in megastring for word in ['smokefree', 'smoke free', 'smoke-free', 'pet free', 'pet-free', 'petfree']):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "books['smokefree'] = books.megastring.apply(smokefree)\n",
    "\n",
    "def hardcover(megastring):\n",
    "    if any(word in megastring for word in ['hard cover', 'hardcover', 'hard-cover']):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "books['hardcover'] = books.megastring.apply(hardcover)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11710"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_subset = books_filtered = books[(books.total_price <= 60) &((books.days_since_posted > 90) & (books.sold_true == 0)) | ((books.days_since_posted <= 90) & (books.sold_true == 1))]\n",
    "books_subset.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['brand', 'category', 'condition', 'description', 'id', 'posted',\n",
       "       'price', 'seller_name', 'shipping', 'sold', 'title', 'shipping_cost',\n",
       "       'shipping_time', 'shipping_location', 'total_price', 'free_shipping',\n",
       "       'brand_included', 'sold_true', 'description_length', 'title_length',\n",
       "       'condition_ordinal', 'days_since_posted', 'megastring', 'awardees',\n",
       "       'amazon', 'topbrands', 'disney', 'americangirl', 'seuss', 'lego',\n",
       "       'sesame', 'leapfrog', 'minecraft', 'marvel', 'ericcarle', 'nickelodean',\n",
       "       'starwars', 'winniethepooh', 'harrypotter', 'wimpykid', 'fisherprice',\n",
       "       'scholastic', 'bundle', 'vintage', 'smokefree', 'hardcover'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SMOTE INFLATED TRAINING DATASET\n",
      "Accuracy of RF classifier on training set: 0.8703\n",
      "Accuracy of RF classifier on test set: 0.8268\n",
      "Random Forest - SMOTE TRAINING DATASET \n",
      " [[2359  465]\n",
      " [  42   62]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   available       0.98      0.84      0.90      2824\n",
      "        sold       0.12      0.60      0.20       104\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      2928\n",
      "   macro avg       0.55      0.72      0.55      2928\n",
      "weighted avg       0.95      0.83      0.88      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#removing the days_since_posted feature because of subsetting\n",
    "feature_names_books = ['condition_ordinal',\n",
    "                       'total_price', \n",
    "#                        'price',\n",
    "                       'free_shipping', \n",
    "                       'brand_included', \n",
    "                       'description_length', \n",
    "#                        'title_length',\n",
    "#                        'disney', \n",
    "#                        'americangirl', \n",
    "#                        'seuss', \n",
    "#                        'lego', \n",
    "#                        'sesame', \n",
    "#                        'leapfrog', \n",
    "#                        'minecraft',\n",
    "#                        'marvel', \n",
    "#                        'ericcarle', \n",
    "#                        'nickelodean', \n",
    "#                        'starwars', \n",
    "#                        'winniethepooh',\n",
    "#                        'harrypotter', \n",
    "#                        'wimpykid', \n",
    "#                        'fisherprice', \n",
    "#                        'scholastic', \n",
    "#                        'bundle',\n",
    "#                        'vintage', \n",
    "#                        'smokefree', \n",
    "#                        'hardcover',\n",
    "#                        'awardees',\n",
    "                         'topbrands',\n",
    "#                         'amazon'\n",
    "                      ]\n",
    "X_books = books_subset[feature_names_books]\n",
    "y_books = books_subset['sold_true']\n",
    "\n",
    "X_train1, X_test_FINAL, y_train1, y_test_FINAL = train_test_split(X_books, y_books, random_state=0)\n",
    "#X_train2, X_test, y_train2, y_test = train_test_split(X_train1, y_train1, random_state=0)\n",
    "\n",
    "# SMOTE and ADASYN TO UPSAMPLE ON TRAINING DATA ONLY\n",
    "X_sm, y_sm = SMOTE(random_state = 0).fit_resample(X_train1, y_train1)\n",
    "\n",
    "# from hyperparameter tuning recommended settings below\n",
    "# {'n_estimators': 800,\n",
    "#  'min_samples_split': 10,\n",
    "#  'min_samples_leaf': 2,\n",
    "#  'max_features': 'sqrt',\n",
    "#  'max_depth': 20,\n",
    "#  'bootstrap': False}\n",
    "\n",
    "rf_smote_train = RandomForestClassifier(max_depth = 4, min_samples_split = 10, n_estimators = 100, random_state=0).fit(X_sm, y_sm)\n",
    "print('\\nSMOTE INFLATED TRAINING DATASET')\n",
    "print('Accuracy of RF classifier on training set: {:.4f}'.format(rf_smote_train.score(X_sm, y_sm)))\n",
    "print('Accuracy of RF classifier on test set: {:.4f}'\n",
    "     .format(rf_smote_train.score(X_test_FINAL, y_test_FINAL)))\n",
    "rf_S_predicted = rf_smote_train.predict(X_test_FINAL)\n",
    "rf_S_confusion = confusion_matrix(y_test_FINAL, rf_S_predicted)\n",
    "print('Random Forest - SMOTE TRAINING DATASET \\n', rf_S_confusion)\n",
    "print(classification_report(y_test_FINAL, rf_S_predicted, target_names=['available', 'sold']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading my toolset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_subset['sold_true'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_books = books_subset[feature_names_books]\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "#y_books = lb.fit_transform(books_subset['sold_true'].values)\n",
    "targets = books_subset['sold_true']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_books, targets, stratify=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train class distribution\n",
      "0    0.966636\n",
      "1    0.033364\n",
      "Name: sold_true, dtype: float64\n",
      "y_test class distribution\n",
      "0    0.96653\n",
      "1    0.03347\n",
      "Name: sold_true, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('y_train class distribution')\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print('y_test class distribution')\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': [3, 5, 10], \n",
    "    'n_estimators' : [100, 300],\n",
    "    'max_depth': [3, 5, 15, 25],\n",
    "    'max_features': [3, 5, 10, 20]\n",
    "}\n",
    "\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_wrapper(refit_score='precision_score'):\n",
    "    \"\"\"\n",
    "    fits a GridSearchCV classifier using refit_score for optimization\n",
    "    prints classifier performance metrics\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    grid_search = GridSearchCV(clf, param_grid, scoring=scorers, refit=refit_score,\n",
    "                           cv=skf, return_train_score=True, n_jobs=-1)\n",
    "    grid_search.fit(X_train.values, y_train.values)\n",
    "\n",
    "    # make the predictions\n",
    "    y_pred = grid_search.predict(X_test.values)\n",
    "\n",
    "    print('Best params for {}'.format(refit_score))\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    # confusion matrix on the test data.\n",
    "    print('\\nConfusion matrix of Random Forest optimized for {} on the test data:'.format(refit_score))\n",
    "    print(pd.DataFrame(confusion_matrix(y_test, y_pred),\n",
    "                 columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True,\n",
      " 'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 4,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 10,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': 0,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf_smote_train.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [True, False],\n",
      " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)\n",
    "\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "    'accuracy_score': make_scorer(accuracy_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  2.0min\n",
      "/Users/mleong/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(scoring = 'recall', estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=0, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_sm, y_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 800,\n",
       " 'min_samples_split': 10,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 20,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great. As we can see above, even though accuracy looks better on the model trained with real uninflated data, the models trained with inflated data get much better recall for the sold data. Precision still isn't that great though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== All AUC Scores ===\n",
      "[0.81111468 0.72598779 0.74092515 0.80613556 0.60762411 0.84893617\n",
      " 0.83776596 0.79397163 0.82269504 0.9       ]\n",
      "\n",
      "\n",
      "=== Mean AUC Score ===\n",
      "Mean AUC Score - Random Forest (with SMOTE):  0.7895156094438104\n"
     ]
    }
   ],
   "source": [
    "# Cross validation scores\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf_cv_score = cross_val_score(rf_smote_train, X_test_FINAL, y_test_FINAL, cv=10, scoring='roc_auc')\n",
    "\n",
    "print(\"=== All AUC Scores ===\")\n",
    "print(rf_cv_score)\n",
    "print('\\n')\n",
    "print(\"=== Mean AUC Score ===\")\n",
    "print(\"Mean AUC Score - Random Forest (with SMOTE): \", rf_cv_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== All AUC Scores ===\n",
      "[0.90597509 0.96801522 0.95501661 0.95672042 0.96763391 0.96622507\n",
      " 0.95798424 0.96733634 0.96096842 0.9626485 ]\n",
      "\n",
      "\n",
      "=== Mean AUC Score ===\n",
      "Mean AUC Score - Random Forest (with SMOTE):  0.9568523821753299\n"
     ]
    }
   ],
   "source": [
    "# Cross validation scores\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf_cv_score = cross_val_score(rf_smote_train, X_sm, y_sm, cv=10, scoring='roc_auc')\n",
    "\n",
    "print(\"=== All AUC Scores ===\")\n",
    "print(rf_cv_score)\n",
    "print('\\n')\n",
    "print(\"=== Mean AUC Score ===\")\n",
    "print(\"Mean AUC Score - Random Forest (with SMOTE): \", rf_cv_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== All AUC Scores ===\n",
      "[0.89480556 0.95804834 0.95498908 0.94513157 0.94308136 0.96235848\n",
      " 0.95960324 0.93540795 0.94562755 0.96545863]\n",
      "\n",
      "\n",
      "=== Mean AUC Score ===\n",
      "Mean AUC Score - Random Forest (with ADASYN):  0.9464511755052678\n"
     ]
    }
   ],
   "source": [
    "# Cross validation scores\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf_cv_score = cross_val_score(rf_adasyn_train, X_ad, y_ad, cv=10, scoring='roc_auc')\n",
    "\n",
    "print(\"=== All AUC Scores ===\")\n",
    "print(rf_cv_score)\n",
    "print('\\n')\n",
    "print(\"=== Mean AUC Score ===\")\n",
    "print(\"Mean AUC Score - Random Forest (with ADASYN): \", rf_cv_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow these AUC scores from cross validation look really good! I need to create an AUC plot to include! \n",
    "=== All AUC Scores ===\n",
    "[0.92420226 0.99231339 0.9944249  0.99324231 0.99734874 0.98448045\n",
    " 0.98617653 0.99258983 0.99289677 0.95697637]\n",
    "\n",
    "\n",
    "=== Mean AUC Score ===\n",
    "Mean AUC Score - Random Forest:  0.9814651543780906"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing our winning model (SMOTE + Random Forest) AUC scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-2e6c739b269d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Compute ROC curve and area the curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_sm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobas_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mtprs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_fpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mtprs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mroc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Classification and ROC analysis\n",
    "\n",
    "# Run classifier with cross-validation and plot ROC curves\n",
    "cv = StratifiedKFold(n_splits=6)\n",
    "classifier = RandomForestClassifier(max_depth = 4, min_samples_split = 10, n_estimators = 100, random_state=0)\n",
    "#classifier = svm.SVC(kernel='linear', probability=True, random_state=random_state)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "from scipy import interp\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "\n",
    "\n",
    "i = 0\n",
    "for train, test in cv.split(X_sm, y_sm):\n",
    "    probas_ = classifier.fit(X_sm[train], y_sm[train]).predict_proba(X_sm[test])\n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_sm[test], probas_[:, 1])\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
    "             label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "    i += 1\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic with SMOTE')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIRST ATTEMPT AT RECALL-PRECISION CURVES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1=0.197 auc=0.130 ap=0.126\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n",
    "# precision-recall curve and f1\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "# predict probabilities\n",
    "probs = rf_smote_train.predict_proba(X_test_FINAL)\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = probs[:, 1]\n",
    "# predict class values\n",
    "yhat = rf_smote_train.predict(X_test_FINAL)\n",
    "# calculate precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test_FINAL, probs)\n",
    "# calculate F1 score\n",
    "f1 = f1_score(y_test_FINAL, yhat)\n",
    "# calculate precision-recall AUC\n",
    "auc = auc(recall, precision)\n",
    "# calculate average precision score\n",
    "ap = average_precision_score(y_test_FINAL, probs)\n",
    "print('f1=%.3f auc=%.3f ap=%.3f' % (f1, auc, ap))\n",
    "# plot no skill\n",
    "pyplot.plot([0, 1], [0.5, 0.5], linestyle='--')\n",
    "# plot the precision-recall curve for the model\n",
    "pyplot.plot(recall, precision, marker='.')\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:  ['condition_ordinal', 'total_price', 'free_shipping', 'brand_included', 'description_length']\n"
     ]
    }
   ],
   "source": [
    "print(\"Column names: \", str(feature_names_books))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-d429f057609e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimportances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_smote_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m std = np.std([tree.feature_importances_ for tree in rf.estimators_],\n\u001b[0m\u001b[1;32m      4\u001b[0m              axis=0)\n\u001b[1;32m      5\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rf' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "importances = rf_smote_train.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "# Pritn features: \n",
    "X_train\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X_train_scaled.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), indices)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strange. Free shipping and including a Brand are some of the most important features. Sklearns features importances has been criticized as being biased because it inflates the importance of continuous or high cardinality variables which isn't the case with either of these features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strange. Free shipping and including a Brand are some of the most important features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL CHECKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "condition_ordinal       1.0\n",
       "total_price            18.0\n",
       "free_shipping           1.0\n",
       "brand_included          0.0\n",
       "description_length    250.0\n",
       "Name: 240, dtype: float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.iloc[3,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "features = [[3,7,1,1,200]]\n",
    "prediction = rf.predict(features)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of RF classifier on ORIGINAL subtest set: 0.9385\n",
      "Random Forest \n",
      " [[2753   90]\n",
      " [  91   10]]\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of RF classifier on ORIGINAL subtest set: {:.4f}'\n",
    "     .format(rf.score(X_test_FINAL, y_test_FINAL)))\n",
    "\n",
    "rf_predicted = rf.predict(X_test_FINAL)\n",
    "rf_confusion = confusion_matrix(y_test_FINAL, rf_predicted)\n",
    "\n",
    "print('Random Forest \\n', rf_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOW TO SAVE MODEL FOR WEB APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(rf_smote_train, open('model.pkl', 'wb'))\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "# some time later...\n",
    "# load the model from disk\n",
    "#loaded_model = pickle.load(open('model.pkl', 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient-Boosted Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(learning_rate=0.01, max_depth=2)\n",
      "Accuracy of GBDT classifier on training set: 0.8257\n",
      "Accuracy of GBDT classifier on test set: 0.8304\n",
      "Gradient-Boosted Decision Trees \n",
      " [[2382  484]\n",
      " [ 481 2344]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbdt = GradientBoostingClassifier(learning_rate = 0.01, max_depth = 2, random_state = 0)\n",
    "gbdt.fit(X_train_scaled, y_train)\n",
    "print('(learning_rate=0.01, max_depth=2)')\n",
    "print('Accuracy of GBDT classifier on training set: {:.4f}'\n",
    "     .format(gbdt.score(X_train_scaled, y_train)))\n",
    "print('Accuracy of GBDT classifier on test set: {:.4f}'\n",
    "     .format(gbdt.score(X_test_scaled, y_test)))\n",
    "\n",
    "gbdt_predicted = gbdt.predict(X_test_scaled)\n",
    "gbdt_confusion = confusion_matrix(y_test, gbdt_predicted)\n",
    "\n",
    "print('Gradient-Boosted Decision Trees \\n', gbdt_confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating these metrics with alternative models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy predictor (most frequent) \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   available       0.50      0.50      0.50      2866\n",
      "        sold       0.50      0.50      0.50      2825\n",
      "\n",
      "   micro avg       0.50      0.50      0.50      5691\n",
      "   macro avg       0.50      0.50      0.50      5691\n",
      "weighted avg       0.50      0.50      0.50      5691\n",
      "\n",
      "KNN \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   available       0.94      0.93      0.93      2866\n",
      "        sold       0.93      0.94      0.93      2825\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      5691\n",
      "   macro avg       0.93      0.93      0.93      5691\n",
      "weighted avg       0.93      0.93      0.93      5691\n",
      "\n",
      "Logistic regression\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   available       0.76      0.79      0.77      2866\n",
      "        sold       0.78      0.75      0.76      2825\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      5691\n",
      "   macro avg       0.77      0.77      0.77      5691\n",
      "weighted avg       0.77      0.77      0.77      5691\n",
      "\n",
      "SVC\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   available       0.77      0.77      0.77      2866\n",
      "        sold       0.77      0.77      0.77      2825\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      5691\n",
      "   macro avg       0.77      0.77      0.77      5691\n",
      "weighted avg       0.77      0.77      0.77      5691\n",
      "\n",
      "Naive Bayes\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   available       0.80      0.65      0.72      2866\n",
      "        sold       0.70      0.84      0.76      2825\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      5691\n",
      "   macro avg       0.75      0.74      0.74      5691\n",
      "weighted avg       0.75      0.74      0.74      5691\n",
      "\n",
      "Random Forest\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   available       0.96      0.97      0.97      2866\n",
      "        sold       0.97      0.96      0.97      2825\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      5691\n",
      "   macro avg       0.97      0.97      0.97      5691\n",
      "weighted avg       0.97      0.97      0.97      5691\n",
      "\n",
      "Gradient-Boosted Decision Trees\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   available       0.83      0.83      0.83      2866\n",
      "        sold       0.83      0.83      0.83      2825\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      5691\n",
      "   macro avg       0.83      0.83      0.83      5691\n",
      "weighted avg       0.83      0.83      0.83      5691\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('Dummy predictor (most frequent) \\n', classification_report(y_test, dummy_majority_predicted, target_names=['available', 'sold']))\n",
    "print('KNN \\n', classification_report(y_test, knn_predicted, target_names=['available', 'sold']))\n",
    "print('Logistic regression\\n', classification_report(y_test, logreg_predicted, target_names = ['available', 'sold']))\n",
    "print('SVC\\n', classification_report(y_test, svc_predicted, target_names = ['available', 'sold']))\n",
    "print('Naive Bayes\\n', classification_report(y_test, nb_predicted, target_names = ['available', 'sold']))\n",
    "print('Random Forest\\n', classification_report(y_test, rf_predicted, target_names = ['available', 'sold']))\n",
    "print('Gradient-Boosted Decision Trees\\n', classification_report(y_test, gbdt_predicted, target_names = ['available', 'sold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49592391304347827\n",
      "Random class-proportional prediction (dummy classifier)\n",
      " [[1074 1067]\n",
      " [  32   35]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_majority = DummyClassifier(strategy = 'stratified').fit(X_train, y_train)\n",
    "print(dummy_majority.score(X_test, y_test))\n",
    "\n",
    "dummy_majority_predicted = dummy_majority.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, dummy_majority_predicted)\n",
    "\n",
    "print('Random class-proportional prediction (dummy classifier)\\n', confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of dummy classifier on ORIGINAL subtest set: 0.4966\n",
      "Dummy Majority \n",
      " [[1362 1481]\n",
      " [  43   58]]\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of dummy classifier on ORIGINAL subtest set: {:.4f}'\n",
    "     .format(dummy_majority.score(X_test1, y_test1)))\n",
    "\n",
    "dummy_majority_predicted = dummy_majority.predict(X_test1_scaled)\n",
    "dummy_majority_confusion = confusion_matrix(y_test1, dummy_majority_predicted)\n",
    "\n",
    "print('Dummy Majority \\n', dummy_majority_confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't think it's reliable to use KNN and SMOTE simultaneously because SMOT may use the KNN method to upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of K-NN classifier on training set: 0.9357\n",
      "Accuracy of K-NN classifier on test set: 0.9065\n",
      "KNN\n",
      " [[3594  372]\n",
      " [ 369 3593]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "print('Accuracy of K-NN classifier on training set: {:.4f}'\n",
    "     .format(knn.score(X_train_scaled, y_train)))\n",
    "print('Accuracy of K-NN classifier on test set: {:.4f}'\n",
    "     .format(knn.score(X_test_scaled, y_test)))\n",
    "\n",
    "knn_predicted = knn.predict(X_test_scaled)\n",
    "knn_confusion = confusion_matrix(y_test, knn_predicted)\n",
    "\n",
    "print('KNN\\n', knn_confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic regression classifier on training set: 0.5654\n",
      "Accuracy of Logistic regression classifier on test set: 0.5711\n",
      "Logistic Regression\n",
      " [[1964 2002]\n",
      " [1398 2564]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(solver = 'lbfgs').fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.4f}'\n",
    "     .format(logreg.score(X_train_scaled, y_train)))\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.4f}'\n",
    "     .format(logreg.score(X_test_scaled, y_test)))\n",
    "\n",
    "logreg_predicted = logreg.predict(X_test_scaled)\n",
    "logreg_confusion = confusion_matrix(y_test, logreg_predicted)\n",
    "\n",
    "print('Logistic Regression\\n', logreg_confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression with Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge regression linear model intercept: 0.311721747825589\n",
      "ridge regression linear model coeff:\n",
      "[0.02179547 0.6513468  0.16073864 0.02921111 0.08021726]\n",
      "R-squared score (training): 0.033\n",
      "R-squared score (test): 0.041\n",
      "Number of non-zero features: 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "linridge = Ridge(alpha=20.0).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linridge.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(X_test_scaled, y_test)))\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linridge.coef_ != 0)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Linear SVC classifier on training set: 0.5649\n",
      "Accuracy of Linear SVC classifier on test set: 0.5706\n",
      "SVC \n",
      " [[1960 2006]\n",
      " [1398 2564]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc = LinearSVC().fit(X_train_scaled, y_train)\n",
    "print('Accuracy of Linear SVC classifier on training set: {:.4f}'\n",
    "     .format(svc.score(X_train_scaled, y_train)))\n",
    "print('Accuracy of Linear SVC classifier on test set: {:.4f}'\n",
    "     .format(svc.score(X_test_scaled, y_test)))\n",
    "\n",
    "svc_predicted = svc.predict(X_test_scaled)\n",
    "svc_confusion = confusion_matrix(y_test, svc_predicted)\n",
    "\n",
    "print('SVC \\n', svc_confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GaussianNB classifier on training set: 0.6091\n",
      "Accuracy of GaussianNB classifier on test set: 0.6126\n",
      "Naive Bayes \n",
      " [[3003  963]\n",
      " [2108 1854]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB().fit(X_train_scaled, y_train)\n",
    "print('Accuracy of GaussianNB classifier on training set: {:.4f}'\n",
    "     .format(nb.score(X_train_scaled, y_train)))\n",
    "print('Accuracy of GaussianNB classifier on test set: {:.4f}'\n",
    "     .format(nb.score(X_test_scaled, y_test)))\n",
    "\n",
    "nb_predicted = nb.predict(X_test_scaled)\n",
    "nb_confusion = confusion_matrix(y_test, nb_predicted)\n",
    "\n",
    "print('Naive Bayes \\n', nb_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
