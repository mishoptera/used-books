{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADVENTURES WITH NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following along https://github.com/priya-dwivedi/Deep-Learning/blob/master/topic_modeling/LDA_Newsgroup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.chdir('/Users/mleong/github/used-books')\n",
    "books = pd.read_csv('used-book-data.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminate duplicate rows\n",
    "books = books.drop_duplicates()\n",
    "\n",
    "# id: convert to string\n",
    "books.id = books.id.astype(str)\n",
    "\n",
    "# posted: convert to datetime\n",
    "books.posted = pd.to_datetime(books.posted)\n",
    "\n",
    "# sold: combine buy now and add to cart categories to be 'for sale'\n",
    "books = books.replace('Buy now', 'available')\n",
    "\n",
    "# price: modify to eliminate '$' sign and convert to number\n",
    "books['price'] = books['price'].str.replace(',', '')\n",
    "books['price'] = books['price'].str.replace('$', '')\n",
    "books['price'] = books['price'].astype(int)\n",
    "\n",
    "# create new column \"shipping_cost\" that is continuous variable from $0 to whatever shipping fee is\n",
    "def shipping_cost(shipping_string):\n",
    "    shipping_after = shipping_string.split(\" | \")\n",
    "    if 'Free' in shipping_after[0]:\n",
    "        shipping_cost = 0\n",
    "    elif '$' in shipping_after[0]:\n",
    "        shipping_cost = shipping_after[0]\n",
    "        shipping_cost = shipping_cost.replace('$', '') \n",
    "    else:\n",
    "        shipping_cost = None\n",
    "    return shipping_cost\n",
    "books['shipping_cost'] = books['shipping'].apply(shipping_cost)\n",
    "books.shipping_cost = pd.to_numeric(books.shipping_cost, errors='coerce')\n",
    "\n",
    "# create new simpler column \"shipping_time\" that is number of days to arrival\n",
    "def shipping_time(shipping_string):\n",
    "    if 'days' in shipping_string:\n",
    "        shipping_after = shipping_string.split(\" | \")\n",
    "        if 'days' in shipping_after[0]:\n",
    "            shipping_time = shipping_after[0]\n",
    "            shipping_time = shipping_time.replace('+ days', '')\n",
    "        elif 'days' in shipping_after[1]:\n",
    "            shipping_time = shipping_after[1]\n",
    "            shipping_time = shipping_time.replace('+ days', '')\n",
    "        else:\n",
    "            shipping_time = None\n",
    "    else:\n",
    "        shipping_time = None\n",
    "    return shipping_time\n",
    "books['shipping_time'] = books['shipping'].apply(shipping_time)\n",
    "\n",
    "# create new simpler column \"shipping_location\" that is just the state product is coming from\n",
    "def shipping_location(shipping_string):\n",
    "    if 'from' in shipping_string:\n",
    "        shipping_after = shipping_string.split(\" | \")\n",
    "        if 'from' in shipping_after[0]:\n",
    "            shipping_location = shipping_after[0]\n",
    "            shipping_location = shipping_location.replace('from ', '')\n",
    "        elif 'from' in shipping_after[1]:\n",
    "            shipping_location = shipping_after[1]\n",
    "            shipping_location = shipping_location.replace('from ', '')\n",
    "        elif 'from' in shipping_after[2]:\n",
    "            shipping_location = shipping_after[2]\n",
    "            shipping_location = shipping_location.replace('from ', '')\n",
    "        else:\n",
    "            shipping_location = None\n",
    "    else:\n",
    "        shipping_location = None\n",
    "    return shipping_location\n",
    "books['shipping_location'] = books['shipping'].apply(shipping_location)\n",
    "\n",
    "# total price: new column that is sum of price and shipping fee\n",
    "books['total_price'] = books['price']+books['shipping_cost']\n",
    "\n",
    "# Making free_shipping column a yes/no\n",
    "def free_shipping(shipping_cost):\n",
    "    if shipping_cost == 0:\n",
    "        free_shipping = 1\n",
    "    else:\n",
    "        free_shipping = 0\n",
    "    return free_shipping\n",
    "books['free_shipping'] = books.shipping_cost.apply(free_shipping)\n",
    "\n",
    "# Making brand_included feature thats just a yes or no\n",
    "books['brand_included'] = books.brand.notnull()\n",
    "zeroANDones = lambda x: x*1\n",
    "books.brand_included = books.brand_included.apply(zeroANDones)\n",
    "\n",
    "# New column sold_true\n",
    "def sold_true(status):\n",
    "    if status == 'SOLD':\n",
    "        sold_true = 1\n",
    "    else:\n",
    "        sold_true = 0\n",
    "    return sold_true\n",
    "books['sold_true'] = books.sold.apply(sold_true)\n",
    "\n",
    "# New column summing of description length\n",
    "def description_length(desc_string):\n",
    "    description_length = len(desc_string)\n",
    "    return description_length\n",
    "books['description_length'] = books.description.apply(description_length)\n",
    "\n",
    "# New column condition_ordinal from new = 1, like new = 2, good = 3, fair = 4, poor = 5\n",
    "def condition_ordinal(condition):\n",
    "    if condition == \"New\":\n",
    "        condition_ordinal = 1\n",
    "    elif condition == \"Like new\":\n",
    "        condition_ordinal = 2\n",
    "    elif condition == \"Good\":\n",
    "        condition_ordinal = 3\n",
    "    elif condition == \"Fair\":\n",
    "        condition_ordinal = 4\n",
    "    elif condition == \"Poor\":\n",
    "        condition_ordinal = 5\n",
    "    else: condition_ordinal = None\n",
    "    return condition_ordinal\n",
    "books['condition_ordinal'] = books.condition.apply(condition_ordinal)\n",
    "\n",
    "# New column days_since_posted. Was scraped on 2019-06-10 so range from 0 and up.\n",
    "from datetime import date\n",
    "\n",
    "def days_since_posted(posted):\n",
    "    posted_day = date(posted.year, posted.month, posted.day)\n",
    "    scrape_day = date(2019, 6, 10)\n",
    "    days_since_posted = scrape_day - posted_day\n",
    "    return days_since_posted.days\n",
    "books['days_since_posted'] = books.posted.apply(days_since_posted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                            Book\n",
       "1                    Book- When Bad Things Happen\n",
       "2                         Usborne Wipe-Clean- NEW\n",
       "3                               Jingle Bells Book\n",
       "4                           My Sister The Vampire\n",
       "5                  Usborne Wipe Clean Pen control\n",
       "6                        Are You My Mother?  Book\n",
       "7                            STAR WARS Color Book\n",
       "8                           My forever dress book\n",
       "9                        Book Disney photomosaics\n",
       "10        The Watson's Go to Birmingham Hardcover\n",
       "11           The Pomegranate Witch Hardcover Book\n",
       "12                                      Baby Book\n",
       "13                                     Kids Books\n",
       "14              Disney's princess collection book\n",
       "15                     Kids Harry Potter Cookbook\n",
       "16                        Paw Patrol My Busy Book\n",
       "17                                  usborne books\n",
       "18                 Courage & Defiance (Paperback)\n",
       "19        Shadow Theater Entertainment Package Gi\n",
       "20                  Junie B. Jones Set Of 3 Books\n",
       "21                         The Books of Elsewhere\n",
       "22                         Nancy Clancy Books-NEW\n",
       "23                         1 Read And Color Books\n",
       "24                        New American girl books\n",
       "25          Usborne Books & More Christmas Bundle\n",
       "26        Star Wars \"Learn To Read\" Level 3\"Dark\"\n",
       "27                 I Saw ESAU Childrens Gift Book\n",
       "28                                  Picture Books\n",
       "29               Madonna's The English Roses Book\n",
       "                           ...                   \n",
       "49349                            3 Dr Seuss Books\n",
       "49350          diary of a wimpy kid The Long Haul\n",
       "49351             Funny Bunnies  Kids Book (1969)\n",
       "49352                        diary of a wimpy kid\n",
       "49353                                       books\n",
       "49354               4 Vintage Little Golden Books\n",
       "49355                      My Little Pony G1 Book\n",
       "49356            Japanese Children's Picture Book\n",
       "49357                              RL Stine Books\n",
       "49358          Return Of The Jedi Story Book 1983\n",
       "49359                                Child's book\n",
       "49360    Riddles from Highlights: Selected by Chi\n",
       "49361                      Usborne Animal Stories\n",
       "49362                Barbies New York Summer Book\n",
       "49363        2 beautiful vintage children's books\n",
       "49364                       The 39 Clues Book Lot\n",
       "49365           1917 Primer & Second Reader Books\n",
       "49366         The Christmas Book Francis X Weiser\n",
       "49367       Clifford The Big Red Dog Vintage Book\n",
       "49368               Paperback X 4 - Young Readers\n",
       "49369      A Treasury Of Little Golden Books 1974\n",
       "49370     Uncle Arthur's Bedtime Stories Lot Of 4\n",
       "49374         diary of a wimpy kid The Last Straw\n",
       "49380      Wonder Books, Easy Reader 11 Hardcover\n",
       "49382                        R. L. Stine Book Lot\n",
       "49387    Vintage Peanuts Charlie Brown book lot 7\n",
       "49399            Japanese Children's Picture Book\n",
       "49421    Vintage Peanuts Charlie Brown book lot 7\n",
       "49476         Lot Of 12 Better Little Books 1940s\n",
       "49477            Burgess Animal Book For Children\n",
       "Name: title, Length: 22134, dtype: object"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books['title']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan pets on the loose series book 1 and 2 pets on the loose series book 1 and 2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "brand = books['brand'][30]\n",
    "title = books['title'][30]\n",
    "description = books['description'][30]\n",
    "megastring = (str(brand) + \" \" + str(title) + \" \" + str(description)).lower()\n",
    "print(megastring)\n",
    "\n",
    "list_of_tokens = [\"Dr. Seuss\"]\n",
    "token = 'Dr. Seuss'\n",
    "\n",
    "for token in list_of_tokens:\n",
    "    if token in megastring: \n",
    "        print(1)\n",
    "    else:\n",
    "        print(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordsearch(brand, title, description):\n",
    "    megastring = (str(brand) + \" \" + str(title) + \" \" + str(description)).lower()\n",
    "    \n",
    "    \n",
    "print(words)\n",
    "print(\"\\n\\nTokenized and lemmatized document: \")\n",
    "print(preprocess(doc_sample))\n",
    "    result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\"\".join(\".Seus\".lower().replace(\".\", \"\").split(\" \"))\n",
    " original_words = ['Disney', 'AmericanGirl', 'Seus', 'lego', 'Sesame', LeapFrog, Minecraft, Marvel, EricCarle, Nickelodeon, StarWars, WinnieThePooh,HarryPotter]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Write a function to perform the pre processing steps on the entire dataset\n",
    "'''\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n",
      "['Would', 'make', 'a', 'great', 'bday', 'gift', 'for', 'younger', 'child', 'or', 'Easter', 'basket', 'filler', 'Brand', 'new.', '\\n*Dr', 'Seuss', 'hard', 'cover', '\"oh', 'the', 'places', 'you', 'go\"\\n*Play,learn,color.', 'Pull', 'back', 'cars', 'and', 'let', 'them', 'go.', '\\n\\nComes', 'from', 'a', 'smoke', 'free', 'home.']\n",
      "\n",
      "\n",
      "Tokenized and lemmatized document: \n",
      "['great', 'bday', 'gift', 'younger', 'child', 'easter', 'basket', 'filler', 'brand', 'seuss', 'hard', 'cover', 'place', 'play', 'learn', 'color', 'pull', 'car', 'come', 'smoke', 'free', 'home']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview a document after preprocessing\n",
    "'''\n",
    "document_num = 50\n",
    "doc_sample = 'Would make a great bday gift for younger child or Easter basket filler Brand new. \\n*Dr Seuss hard cover \"oh the places you go\"\\n*Play,learn,color. Pull back cars and let them go. \\n\\nComes from a smoke free home.'\n",
    "\n",
    "print(\"Original document: \")\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print(\"\\n\\nTokenized and lemmatized document: \")\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Would make a great bday gift for younger child or Easter basket filler Brand new. \\n*Dr Seuss hard cover \"oh the places you go\"\\n*Play,learn,color. Pull back cars and let them go. \\n\\nComes from a smoke free home.'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books['description'][90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "decoding to str: need a bytes-like object, int found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-368cf67b226a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprocessed_docs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-96-d19ef216e4d9>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimple_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTOPWORDS\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatize_stemming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36msimple_preprocess\u001b[0;34m(doc, deacc, min_len, max_len)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \"\"\"\n\u001b[1;32m    304\u001b[0m     tokens = [\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeacc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeacc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmin_len\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     ]\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(text, lowercase, deacc, encoding, errors, to_lower, lower)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \"\"\"\n\u001b[1;32m    255\u001b[0m     \u001b[0mlowercase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlowercase\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mto_lower\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36many2unicode\u001b[0;34m(text, encoding, errors)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: decoding to str: need a bytes-like object, int found"
     ]
    }
   ],
   "source": [
    "processed_docs = []\n",
    "\n",
    "for doc in books['title']:\n",
    "    processed_docs.append(preprocess(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], []]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Preview 'processed_docs'\n",
    "'''\n",
    "print(processed_docs[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3: Bag of words on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a dictionary from 'processed_docs' containing the number of times a word appears \n",
    "in the training set using gensim.corpora.Dictionary and call it 'dictionary'\n",
    "'''\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 addit\n",
      "1 bodi\n",
      "2 bricklin\n",
      "3 bring\n",
      "4 bumper\n",
      "5 call\n",
      "6 colleg\n",
      "7 door\n",
      "8 earli\n",
      "9 engin\n",
      "10 enlighten\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Checking dictionary created\n",
    "'''\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "OPTIONAL STEP\n",
    "Remove very rare and very common words:\n",
    "\n",
    "- words appearing less than 15 times\n",
    "- words appearing in more than 10% of all documents\n",
    "'''\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 addit\n",
      "1 bodi\n",
      "2 bring\n",
      "3 bumper\n",
      "4 call\n",
      "5 colleg\n",
      "6 door\n",
      "7 earli\n",
      "8 engin\n",
      "9 enlighten\n",
      "10 histori\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Checking dictionary created\n",
    "'''\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create the Bag-of-words model for each document i.e for each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to 'bow_corpus'\n",
    "'''\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 18 (\"rest\") appears 1 time.\n",
      "Word 166 (\"clear\") appears 1 time.\n",
      "Word 336 (\"refer\") appears 1 time.\n",
      "Word 350 (\"true\") appears 1 time.\n",
      "Word 391 (\"technolog\") appears 1 time.\n",
      "Word 437 (\"christian\") appears 1 time.\n",
      "Word 453 (\"exampl\") appears 1 time.\n",
      "Word 476 (\"jew\") appears 1 time.\n",
      "Word 480 (\"lead\") appears 1 time.\n",
      "Word 482 (\"littl\") appears 3 time.\n",
      "Word 520 (\"wors\") appears 2 time.\n",
      "Word 721 (\"keith\") appears 3 time.\n",
      "Word 732 (\"punish\") appears 1 time.\n",
      "Word 803 (\"california\") appears 1 time.\n",
      "Word 859 (\"institut\") appears 1 time.\n",
      "Word 917 (\"similar\") appears 1 time.\n",
      "Word 990 (\"allan\") appears 1 time.\n",
      "Word 991 (\"anti\") appears 1 time.\n",
      "Word 992 (\"arriv\") appears 1 time.\n",
      "Word 993 (\"austria\") appears 1 time.\n",
      "Word 994 (\"caltech\") appears 2 time.\n",
      "Word 995 (\"distinguish\") appears 1 time.\n",
      "Word 996 (\"german\") appears 1 time.\n",
      "Word 997 (\"germani\") appears 3 time.\n",
      "Word 998 (\"hitler\") appears 1 time.\n",
      "Word 999 (\"livesey\") appears 2 time.\n",
      "Word 1000 (\"motto\") appears 2 time.\n",
      "Word 1001 (\"order\") appears 1 time.\n",
      "Word 1002 (\"pasadena\") appears 1 time.\n",
      "Word 1003 (\"pompous\") appears 1 time.\n",
      "Word 1004 (\"popul\") appears 1 time.\n",
      "Word 1005 (\"rank\") appears 1 time.\n",
      "Word 1006 (\"schneider\") appears 1 time.\n",
      "Word 1007 (\"semit\") appears 1 time.\n",
      "Word 1008 (\"social\") appears 1 time.\n",
      "Word 1009 (\"solntz\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview BOW for our sample preprocessed document\n",
    "'''\n",
    "document_num = 20\n",
    "bow_doc_x = bow_corpus[document_num]\n",
    "\n",
    "for i in range(len(bow_doc_x)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
    "                                                     dictionary[bow_doc_x[i][0]], \n",
    "                                                     bow_doc_x[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4: Running LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA mono-core -- fallback code in case LdaMulticore throws an error on your machine\n",
    "# lda_model = gensim.models.LdaModel(bow_corpus, \n",
    "#                                    num_topics = 10, \n",
    "#                                    id2word = dictionary,                                    \n",
    "#                                    passes = 50)\n",
    "\n",
    "# LDA multicore \n",
    "'''\n",
    "Train your lda model using gensim.models.LdaMulticore and save it to 'lda_model'\n",
    "'''\n",
    "# TODO\n",
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = 8, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.007*\"presid\" + 0.005*\"clinton\" + 0.004*\"homosexu\" + 0.004*\"netcom\" + 0.004*\"virginia\" + 0.004*\"bike\" + 0.004*\"run\" + 0.003*\"pitch\" + 0.003*\"talk\" + 0.003*\"consid\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.009*\"govern\" + 0.007*\"armenian\" + 0.006*\"israel\" + 0.005*\"kill\" + 0.005*\"isra\" + 0.004*\"american\" + 0.004*\"turkish\" + 0.004*\"weapon\" + 0.004*\"jew\" + 0.004*\"countri\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.017*\"game\" + 0.015*\"team\" + 0.011*\"play\" + 0.009*\"player\" + 0.008*\"hockey\" + 0.006*\"season\" + 0.005*\"leagu\" + 0.005*\"canada\" + 0.005*\"score\" + 0.004*\"andrew\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.012*\"window\" + 0.011*\"card\" + 0.008*\"drive\" + 0.007*\"driver\" + 0.006*\"sale\" + 0.005*\"control\" + 0.005*\"scsi\" + 0.005*\"disk\" + 0.005*\"speed\" + 0.005*\"price\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.013*\"file\" + 0.009*\"program\" + 0.007*\"window\" + 0.006*\"encrypt\" + 0.006*\"chip\" + 0.006*\"imag\" + 0.006*\"data\" + 0.006*\"avail\" + 0.005*\"code\" + 0.004*\"version\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.012*\"space\" + 0.009*\"nasa\" + 0.006*\"scienc\" + 0.005*\"orbit\" + 0.004*\"research\" + 0.004*\"launch\" + 0.003*\"pitt\" + 0.003*\"food\" + 0.003*\"earth\" + 0.003*\"develop\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.022*\"drive\" + 0.007*\"car\" + 0.006*\"hard\" + 0.005*\"uiuc\" + 0.005*\"columbia\" + 0.005*\"engin\" + 0.005*\"bike\" + 0.004*\"light\" + 0.004*\"colorado\" + 0.004*\"navi\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.012*\"christian\" + 0.008*\"jesus\" + 0.006*\"exist\" + 0.005*\"moral\" + 0.005*\"bibl\" + 0.005*\"word\" + 0.005*\"religion\" + 0.004*\"church\" + 0.004*\"life\" + 0.004*\"claim\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Brand Categories on Children's book site that we can see if appear in Brand, Title, or Description Categories.\n",
    "\n",
    "Scholastic\n",
    "Disney\n",
    "American Girl\n",
    "Vintage\n",
    "Dr. Seuss\n",
    "HarperCollins Publishers\n",
    "Random House\n",
    "Sesame Street\n",
    "Leap Frog\n",
    "ABRAMS\n",
    "Osborne\n",
    "Minecraft\n",
    "Marvel\n",
    "Lot\n",
    "Eric Carle\n",
    "Aladdin\n",
    "Nickelodeon\n",
    "Hallmark\n",
    "LEGO\n",
    "Custom Variety Pack\n",
    "Star Wars\n",
    "Winnie The Pooh\n",
    "Fisher-Price\n",
    "DK Publishing\n",
    "National Geographic\n",
    "Disney Princess\n",
    "MacMillan Childrens\n",
    "Harry Potter\n",
    "Penguin\n",
    "Disney Pixar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Ideas for two sets of categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "brand =\n",
    "\n",
    "Publisher = ['Scholastic', 'HarperCollins', 'RandomHouse', 'Hallmark', 'DKPublishing', 'FisherPrice', 'Macillin', 'Peanguin'\n",
    "\n",
    "descriptor = ['bundle', 'vintage', 'smokefree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
